{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pG9eugOTJYNO"},"outputs":[],"source":["# importing necessary libraries\n","\n","import nltk, re, string\n","from nltk.corpus import stopwords, twitter_samples\n","import numpy as np\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CkaGbJoUJoOC"},"outputs":[],"source":["# Preprocessing of the tweets that is our data\n","\n","def process_tweet(tweet):\n","    stemmer = nltk.PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","    tweet = re.sub(r'#', '', tweet)\n","    tokenizer = nltk.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","        if (word not in stopwords_english and\n","                word not in string.punctuation):\n","            stem_word = stemmer.stem(word)  # stemming word\n","            tweets_clean.append(stem_word)\n","\n","    return tweets_clean\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKakfgACJwQL"},"outputs":[],"source":["# This is the most important part of the whole code: \n","# The reason is our feature set through which we will be training our model on will be build here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPTgml3SJ0pJ"},"outputs":[],"source":["def build_freqs(tweets, ys):\n","    \"\"\"Build frequencies.\n","    Input:\n","        tweets: a list of tweets\n","        ys: an m x 1 array with the sentiment label of each tweet\n","            (either 0 or 1)\n","    Output:\n","        freqs: a dictionary mapping each (word, sentiment) pair to its\n","        frequency\n","    \"\"\"\n","    # Convert np array to list since zip needs an iterable.\n","    # The squeeze is necessary or the list ends up with one element.\n","    # Also note that this is just a NOP if ys is already a list.\n","    yslist = np.squeeze(ys).tolist()\n","\n","    # Start with an empty dictionary and populate it by looping over all tweets\n","    # and over all processed words in each tweet.\n","    freqs = {}\n","    for y, tweet in zip(yslist, tweets):\n","        for word in process_tweet(tweet):\n","            pair = (word, y)\n","            if pair in freqs:\n","                freqs[pair] += 1\n","            else:\n","                freqs[pair] = 1\n","\n","    return freqs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":510,"status":"ok","timestamp":1622362224413,"user":{"displayName":"siddharth patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjnwsdj5DKa95WHRl25dlQz9L1Bbc10vap4xTNIiA=s64","userId":"01966341811263607507"},"user_tz":-330},"id":"Y61jYX3fUkkU","outputId":"2fc1c5a3-791f-4cc6-d997-d61193ee40be"},"outputs":[{"name":"stdout","output_type":"stream","text":["{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}\n"]}],"source":["# Checking how the above code works with an example.\n","\n","tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n","ys = [1, 0, 0, 0, 0]\n","res = build_freqs(tweets, ys)\n","print(res)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_aKmKS4-KJvp"},"outputs":[],"source":["# nltk.download('twitter_samples')\n","# nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxViCeI-KDN7"},"outputs":[],"source":["# select the set of positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03NIPdv1KF96"},"outputs":[],"source":["# split the data into two pieces, one for training and one for testing.\n","test_pos = all_positive_tweets[4000:]\n","train_pos = all_positive_tweets[:4000]\n","test_neg = all_negative_tweets[4000:]\n","train_neg = all_negative_tweets[:4000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MOGG2-3WKRQ7"},"outputs":[],"source":["train_x = train_pos + train_neg\n","test_x = test_pos + test_neg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGQdgF5VKTia"},"outputs":[],"source":["# combine positive and negative labels\n","# We are building our y - target variable here\n","train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n","test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVDUs1paKb4z"},"outputs":[],"source":["# create frequency dictionary\n","freqs = build_freqs(train_x, train_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1622359584610,"user":{"displayName":"siddharth patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjnwsdj5DKa95WHRl25dlQz9L1Bbc10vap4xTNIiA=s64","userId":"01966341811263607507"},"user_tz":-330},"id":"IV5_4qVUKewa","outputId":"2452a25e-60e3-4e98-81c8-3d3c7205f3a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["type(freqs) = <class 'dict'>\n","len(freqs) = 11346\n"]}],"source":["# check out the output\n","\n","print(\"type(freqs) = \" + str(type(freqs)))\n","print(\"len(freqs) = \" + str(len(freqs.keys())))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":359,"status":"ok","timestamp":1622359619111,"user":{"displayName":"siddharth patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjnwsdj5DKa95WHRl25dlQz9L1Bbc10vap4xTNIiA=s64","userId":"01966341811263607507"},"user_tz":-330},"id":"ZyVQ7YMoKp2T","outputId":"afcc5ef9-63f7-483c-ba67-ffbc682164a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["This is an example of a positive tweet: \n"," @gculloty87 Yeah I suppose she was lol! Chat in a bit just off out x :))\n","\n","This is an example of the processed version of the tweet: \n"," ['yeah', 'suppos', 'lol', 'chat', 'bit', 'x', ':)']\n"]}],"source":["# test the function below\n","\n","print('This is an example of a positive tweet: \\n', train_x[22])\n","print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[22]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCaQzncSKwL_"},"outputs":[],"source":["# Unlike most, we will actully build a Logistic Regression Model from scratch\n","#  Logistic regression\n","\n","# Sigmoid Function\n","def sigmoid(z):\n","    \"\"\"\n","    Input:\n","        z: is the input (can be a scalar or an array)\n","    Output:\n","        h: the sigmoid of z\n","    \"\"\"\n","    zz = np.negative(z)\n","    h = 1 / (1 + np.exp(zz))\n","    return h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ARg-_KLtK-ZX"},"outputs":[],"source":["# Cost function and Gradient\n","def gradientDescent(x, y, theta, alpha, num_iters):\n","    \"\"\"\n","    Input:\n","        x: matrix of features which is (m,n+1)\n","        y: corresponding labels of the input matrix x, dimensions (m,1)\n","        theta: weight vector of dimension (n+1,1)\n","        alpha: learning rate\n","        num_iters: number of iterations you want to train your model for\n","    Output:\n","        J: the final cost\n","        theta: your final weight vector\n","    Hint: you might want to print the cost to make sure that it is going down.\n","    \"\"\"\n","    # get 'm', the number of rows in matrix x\n","    m = x.shape[0]\n","    for i in range(0, num_iters):\n","        z = np.dot(x, theta)\n","        h = sigmoid(z)\n","        # calculate the cost function\n","        cost = -1. / m * (np.dot(y.transpose(), np.log(h)) + np.dot((1 - y).transpose(), np.log(1 - h)))\n","        # update the weights theta\n","        theta = theta - (alpha / m) * np.dot(x.transpose(), (h - y))\n","\n","    cost = float(cost)\n","    return cost, theta\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3RneTodLBcC"},"outputs":[],"source":["#  Extracting the features\n","\n","def extract_features(tweet, freqs):\n","    \"\"\"\n","    Input:\n","        tweet: a list of words for one tweet\n","        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n","    Output:\n","        x: a feature vector of dimension (1,3)\n","    \"\"\"\n","\n","    word_l = process_tweet(tweet)\n","    x = np.zeros((1, 3))\n","\n","    # bias term is set to 1\n","    x[0, 0] = 1\n","\n","    for word in word_l:\n","        # increment the word count for the positive label 1\n","        x[0, 1] += freqs.get((word, 1.0), 0)\n","        # increment the word count for the negative label 0\n","        x[0, 2] += freqs.get((word, 0.0), 0)\n","\n","    assert (x.shape == (1, 3))\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1622359716199,"user":{"displayName":"siddharth patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjnwsdj5DKa95WHRl25dlQz9L1Bbc10vap4xTNIiA=s64","userId":"01966341811263607507"},"user_tz":-330},"id":"dSTFB_uCLEdz","outputId":"aa24fe51-efc3-421d-e039-31f622e188b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.000e+00 3.006e+03 1.240e+02]]\n"]}],"source":["# test on training data\n","\n","tmp1 = extract_features(train_x[22], freqs)\n","print(tmp1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRFwSxLILIDI"},"outputs":[],"source":["# Try to understand what all these three numbers mean. \n","# Usually we get a dataset with a lot of features/columns, here we just have text data.\n","# Those three numbers are the feature set that we have build using build_freq() and extract_features() function.\n","# build_freq() builds a dictionary having words as keys and the number of times they have occurred in corpus as values.\n","# Extract feature takes in sum of these values for positive and negative words, i.e. tmp1[1] and tmp[2]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eiHmtVaIN5KB"},"outputs":[],"source":["# How these features will be used to predict in Logistic Regression\n","\n","# First a hypothesis is build which for our case will be h(x) = b1 + b2*x1 + b3*x2\n","# here b1 = 1, b2 and b3 are determined by cost and gradient function, x1 and x2 are the positive and negative words feature set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lUutIScOWj7"},"outputs":[],"source":["# Training Your Model\n","\n","# collect the features 'x' and stack them into a matrix 'X'\n","X = np.zeros((len(train_x), 3))\n","for i in range(len(train_x)):\n","    X[i, :] = extract_features(train_x[i], freqs)\n","\n","# training labels corresponding to X\n","Y = train_y\n","\n","# Apply gradient descent\n","# these values are predefined (Andrew NG)\n","J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUJ-lXxfOcvJ"},"outputs":[],"source":["def predict_tweet(tweet, freqs, theta):\n","    \"\"\"\n","    Input:\n","        tweet: a string\n","        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n","        theta: (3,1) vector of weights\n","    Output:\n","        y_pred: the probability of a tweet being positive or negative\n","    \"\"\"\n","    # extract the features of the tweet and store it into x\n","    x = extract_features(tweet, freqs)\n","    y_pred = sigmoid(np.dot(x, theta))\n","\n","    return y_pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDQP49HuOhjt"},"outputs":[],"source":["def test_logistic_regression(test_x, test_y, freqs, theta):\n","    \"\"\"\n","    Input:\n","        test_x: a list of tweets\n","        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n","        freqs: a dictionary with the frequency of each pair (or tuple)\n","        theta: weight vector of dimension (3, 1)\n","    Output:\n","        accuracy: (# of tweets classified correctly) / (total # of tweets)\n","    \"\"\"\n","    # the list for storing predictions\n","    y_hat = []\n","\n","    for tweet in test_x:\n","        # get the label prediction for the tweet\n","        y_pred = predict_tweet(tweet, freqs, theta)\n","        if y_pred > 0.5:\n","            y_hat.append(1)\n","        else:\n","            y_hat.append(0)\n","\n","    accuracy = (y_hat == np.squeeze(test_y)).sum() / len(test_x)\n","\n","    return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1316,"status":"ok","timestamp":1622361634031,"user":{"displayName":"siddharth patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjnwsdj5DKa95WHRl25dlQz9L1Bbc10vap4xTNIiA=s64","userId":"01966341811263607507"},"user_tz":-330},"id":"9-B599jISbFd","outputId":"45f6d6e3-4303-4004-9a43-549f5dc803d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Logistic regression model's accuracy = 0.9950\n"]}],"source":["tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n","print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1RuRdapSdYB"},"outputs":[],"source":["# Predict with your own tweet\n","\n","def pre(sentence):\n","    yhat = predict_tweet(sentence, freqs, theta)\n","    if yhat > 0.5:\n","        return 'Positive sentiment'\n","    elif yhat == 0:\n","        return 'Neutral sentiment'\n","    else:\n","        return 'Negative sentiment'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":360,"status":"ok","timestamp":1622361716855,"user":{"displayName":"siddharth patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjnwsdj5DKa95WHRl25dlQz9L1Bbc10vap4xTNIiA=s64","userId":"01966341811263607507"},"user_tz":-330},"id":"UfaCSbBzSoG7","outputId":"3f476d64-566b-4041-b67f-27c178c8e30a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Positive sentiment\n"]}],"source":["my_tweet = 'It is so hot today but it is the perfect day for a beach party'\n","\n","res = pre(my_tweet)\n","print(res)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LdyAf8mSs0P"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPhCtoyBwrvd2ZHTmIihM2m","collapsed_sections":[],"name":"Sentiment LG.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
